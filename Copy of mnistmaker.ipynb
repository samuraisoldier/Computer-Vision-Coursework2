{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of mnistmaker.ipynb","version":"0.3.2","provenance":[{"file_id":"1xmithfeSWFaOQGV5g7UBpurZGNhDUpb7","timestamp":1552654089398}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"8qz1vZ_wUegg","colab_type":"code","outputId":"c3f81ee7-99ff-4951-a15c-ec68d654684d","executionInfo":{"status":"ok","timestamp":1552654310861,"user_tz":0,"elapsed":37106,"user":{"displayName":"Harshil Sumaria","photoUrl":"","userId":"10113880282364149844"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["import os, time, itertools, imageio, pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n","# leaky_relu\n","def lrelu(X, leak=0.2):\n","    f1 = 0.5 * (1 + leak)\n","    f2 = 0.5 * (1 - leak)\n","    return f1 * X + f2 * tf.abs(X)\n","\n","# G(z)\n","def generator(x, y, isTrain=True, reuse=False):\n","    with tf.variable_scope('generator', reuse=reuse):\n","        w_init = tf.contrib.layers.xavier_initializer()\n","\n","        cat1 = tf.concat([x, y], 1)\n","\n","        dense1 = tf.layers.dense(cat1, 128, kernel_initializer=w_init)\n","        relu1 = tf.nn.relu(dense1)\n","        \n","        dense2 = tf.layers.dense(relu1, 256, kernel_initializer=w_init)\n","        relu2 = tf.nn.relu(dense2)\n","\n","        dense3 = tf.layers.dense(relu2, 784, kernel_initializer=w_init)\n","        o = tf.nn.tanh(dense3)\n","\n","        return o\n","\n","# D(x)\n","def discriminator(x, y, isTrain=True, reuse=False):\n","    with tf.variable_scope('discriminator', reuse=reuse):\n","        w_init = tf.contrib.layers.xavier_initializer()\n","\n","        cat1 = tf.concat([x, y], 1)\n","\n","        dense1 = tf.layers.dense(cat1, 128, kernel_initializer=w_init)\n","        lrelu1 = lrelu(dense1, 0.2)\n","\n","        dense2 = tf.layers.dense(lrelu1, 32, kernel_initializer=w_init)\n","        lrelu2 = lrelu(dense2, 0.2)\n","        \n","        dense3 = tf.layers.dense(lrelu2, 1, kernel_initializer=w_init)\n","        o = tf.nn.sigmoid(dense3)\n","\n","        return o, dense3\n","\n","# label preprocess\n","onehot = np.eye(10)\n","\n","temp_z_ = np.random.normal(0, 1, (10, 100))\n","fixed_z_ = temp_z_\n","fixed_y_ = np.zeros((10, 1))\n","\n","for i in range(9):\n","    fixed_z_ = np.concatenate([fixed_z_, temp_z_], 0)\n","    temp = np.ones((10,1)) + i\n","    fixed_y_ = np.concatenate([fixed_y_, temp], 0)\n","\n","fixed_y_ = onehot[fixed_y_.astype(np.int32)].squeeze()\n","def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n","    test_images = sess.run(G_z, {z: fixed_z_, y: fixed_y_, isTrain: False})\n","\n","    size_figure_grid = 10\n","    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n","    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n","        ax[i, j].get_xaxis().set_visible(False)\n","        ax[i, j].get_yaxis().set_visible(False)\n","\n","    for k in range(size_figure_grid*size_figure_grid):\n","        i = k // size_figure_grid\n","        j = k % size_figure_grid\n","        ax[i, j].cla()\n","        ax[i, j].imshow(np.reshape(test_images[k], (28, 28)), cmap='gray')\n","\n","    label = 'Epoch {0}'.format(num_epoch)\n","    fig.text(0.5, 0.04, label, ha='center')\n","\n","    if save:\n","        plt.savefig(path)\n","\n","    if show:\n","        plt.show()\n","    else:\n","        plt.close()\n","\n","def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n","    x = range(len(hist['D_losses']))\n","\n","    y1 = hist['D_losses']\n","    y2 = hist['G_losses']\n","\n","    plt.plot(x, y1, label='D_loss')\n","    plt.plot(x, y2, label='G_loss')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","\n","    plt.legend(loc=4)\n","    plt.grid(True)\n","    plt.tight_layout()\n","\n","    if save:\n","        plt.savefig(path)\n","\n","    if show:\n","        plt.show()\n","    else:\n","        plt.close()\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"HptW9mc5UhQL","colab_type":"code","outputId":"fb1453c7-cc2c-454d-c823-cf76af2a53c5","executionInfo":{"status":"ok","timestamp":1552654458458,"user_tz":0,"elapsed":6279,"user":{"displayName":"Harshil Sumaria","photoUrl":"","userId":"10113880282364149844"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"cell_type":"code","source":["# training parameters\n","batch_size = 25\n","glr = 0.002\n","dlr = 0.002\n","train_epoch = 200\n","\n","# load MNIST\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","train_set = (mnist.train.images - 0.5) / 0.5  # normalization; range: -1 ~ 1\n","train_label = mnist.train.labels\n","\n","# variables : input\n","x = tf.placeholder(tf.float32, shape=(None, 784))\n","y = tf.placeholder(tf.float32, shape=(None, 10))\n","z = tf.placeholder(tf.float32, shape=(None, 100))\n","isTrain = tf.placeholder(dtype=tf.bool)\n","\n","# networks : generator\n","G_z = generator(z, y, isTrain)\n","\n","# networks : discriminator\n","D_real, D_real_logits = discriminator(x, y, isTrain)\n","D_fake, D_fake_logits = discriminator(G_z, y, isTrain, reuse=True)\n","\n","# loss for each network\n","D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, 1])))\n","D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, 1])))\n","D_loss = D_loss_real + D_loss_fake\n","G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, 1])))\n","\n","# trainable variables for each network\n","T_vars = tf.trainable_variables()\n","D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n","G_vars = [var for var in T_vars if var.name.startswith('generator')]\n","\n","# optimizer for each network\n","with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n","    D_optim = tf.train.AdamOptimizer(dlr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n","    G_optim = tf.train.AdamOptimizer(glr, beta1=0.5).minimize(G_loss, var_list=G_vars)\n","\n","# open session and initialize all variables\n","sess = tf.InteractiveSession()\n","tf.global_variables_initializer().run()\n","\n","# results save folder\n","name = 'finalimgs'\n","root = F\"/content/gdrive/My Drive/cGANs\"\n","model = 'MNIST_cGANog_'\n","if not os.path.isdir(root):\n","    os.mkdir(root)\n","if not os.path.isdir(root + name + 'Fixed_results'):\n","    os.mkdir(root + name + 'Fixed_results')\n","    \n","train_hist = {}\n","train_hist['D_losses'] = []\n","train_hist['G_losses'] = []\n","train_hist['per_epoch_ptimes'] = []\n","train_hist['total_ptime'] = []"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-2-7020baf455d5>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use urllib or similar directly.\n","Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting MNIST_data/train-images-idx3-ubyte.gz\n","Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting MNIST_data/train-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.one_hot on tensors.\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From <ipython-input-1-a6deac0a66cd>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"D4HtW2EYUm-A","colab_type":"code","outputId":"44e015ca-e276-418c-9509-41d5e801ea58","executionInfo":{"status":"ok","timestamp":1552660704541,"user_tz":0,"elapsed":499471,"user":{"displayName":"Harshil Sumaria","photoUrl":"","userId":"10113880282364149844"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"cell_type":"code","source":["# training-loop\n","train_epoch = 25\n","np.random.seed(int(time.time()))\n","print('training start!')\n","start_time = time.time()\n","for epoch in range(train_epoch):\n","    G_losses = []\n","    D_losses = []\n","    epoch_start_time = time.time()\n","    for iter in range(len(train_set) // batch_size):\n","        # update discriminator\n","        x_ = train_set[iter * batch_size:(iter + 1) * batch_size]\n","        y_ = train_label[iter * batch_size:(iter + 1) * batch_size]\n","\n","        z_ = np.random.normal(0, 1, (batch_size, 100))\n","        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, y: y_, z: z_, isTrain: True})\n","        D_losses.append(loss_d_)\n","\n","        # update generator\n","        z_ = np.random.normal(0, 1, (batch_size, 100))\n","        y_ = np.random.randint(0, 9, (batch_size, 1))\n","        y_ = onehot[y_.astype(np.int32)].squeeze()\n","        loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, y: y_, isTrain: True})\n","        G_losses.append(loss_g_)\n","        \n","#         # update generator\n","#         z_ = np.random.normal(0, 1, (batch_size, 100))\n","#         y_ = np.random.randint(0, 9, (batch_size, 1))\n","#         y_ = onehot[y_.astype(np.int32)].squeeze()\n","#         loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, y: y_, isTrain: True})\n","#         G_losses.append(loss_g_)\n","\n","    epoch_end_time = time.time()\n","    per_epoch_ptime = epoch_end_time - epoch_start_time\n","    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n","    fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n","    show_result((epoch + 1), save=True, path=fixed_p)\n","    train_hist['D_losses'].append(np.mean(D_losses))\n","    train_hist['G_losses'].append(np.mean(G_losses))\n","    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n","\n","end_time = time.time()\n","total_ptime = end_time - start_time\n","train_hist['total_ptime'].append(total_ptime)\n","\n","print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n","print(\"Training finish!... save training results\")\n","with open(root + model + 'train_hist.pkl', 'wb') as f:\n","    pickle.dump(train_hist, f)\n","\n","show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["training start!\n","[1/25] - ptime: 15.00 loss_d: 0.836, loss_g: 1.631\n","[2/25] - ptime: 14.98 loss_d: 0.839, loss_g: 1.640\n","[3/25] - ptime: 15.15 loss_d: 0.839, loss_g: 1.639\n","[4/25] - ptime: 14.91 loss_d: 0.846, loss_g: 1.609\n","[5/25] - ptime: 14.95 loss_d: 0.860, loss_g: 1.572\n","[6/25] - ptime: 15.00 loss_d: 0.834, loss_g: 1.613\n","[7/25] - ptime: 15.04 loss_d: 0.830, loss_g: 1.662\n","[8/25] - ptime: 14.93 loss_d: 0.844, loss_g: 1.605\n","[9/25] - ptime: 14.92 loss_d: 0.845, loss_g: 1.617\n","[10/25] - ptime: 15.08 loss_d: 0.832, loss_g: 1.642\n","[11/25] - ptime: 15.00 loss_d: 0.837, loss_g: 1.631\n","[12/25] - ptime: 14.88 loss_d: 0.819, loss_g: 1.656\n","[13/25] - ptime: 15.18 loss_d: 0.820, loss_g: 1.671\n","[14/25] - ptime: 14.87 loss_d: 0.822, loss_g: 1.671\n","[15/25] - ptime: 14.98 loss_d: 0.796, loss_g: 1.794\n","[16/25] - ptime: 14.95 loss_d: 0.832, loss_g: 1.673\n","[17/25] - ptime: 14.89 loss_d: 0.836, loss_g: 1.649\n","[18/25] - ptime: 15.32 loss_d: 0.813, loss_g: 1.680\n","[19/25] - ptime: 14.89 loss_d: 0.796, loss_g: 1.738\n","[20/25] - ptime: 14.88 loss_d: 0.821, loss_g: 1.684\n","[21/25] - ptime: 14.91 loss_d: 0.845, loss_g: 1.625\n","[22/25] - ptime: 14.88 loss_d: 0.841, loss_g: 1.606\n","[23/25] - ptime: 14.91 loss_d: 0.819, loss_g: 1.677\n","[24/25] - ptime: 14.99 loss_d: 0.813, loss_g: 1.693\n","[25/25] - ptime: 14.93 loss_d: 0.825, loss_g: 1.657\n","Avg per epoch ptime: 14.98, total 25 epochs ptime: 498.24\n","Training finish!... save training results\n"],"name":"stdout"}]},{"metadata":{"id":"8M6eN6PtUqNC","colab_type":"code","colab":{}},"cell_type":"code","source":["# images = []\n","# for e in range(train_epoch):\n","#     img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n","#     images.append(imageio.imread(img_name))\n","# imageio.mimsave(root + model + 'generation_animationdisc+.gif', images, fps=5)\n","\n","# sess.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z1EHms-ueiK-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":350},"outputId":"707f41b9-ef65-4aa0-9328-f67b40e55497","executionInfo":{"status":"error","timestamp":1552661193801,"user_tz":0,"elapsed":459961,"user":{"displayName":"Harshil Sumaria","photoUrl":"","userId":"10113880282364149844"}}},"cell_type":"code","source":["nimagest=60000\n","nimages = 10\n","\n","ni = nimagest/nimages\n","\n","for nii in range(int(ni)):\n","    nwanted = np.random.randint(0,10)\n","    temp = nwanted*np.ones((nimages,1))\n","    fixed_y_ = onehot[temp.astype(np.int32)].squeeze()\n","    test_images=sess.run(G_z, {z: np.random.normal(0, 1, (nimages, 100)), y: fixed_y_, isTrain: False})\n","\n","    for i in range(nimages):\n","        if ((i==0) and (nii==0)):\n","            newdata = np.array(test_images[i])\n","            newlabels = np.eye(10)[nwanted]\n","        else:\n","            newdata = np.vstack((newdata,test_images[i]))\n","            newlabels = np.vstack((newlabels,np.eye(10)[nwanted]))\n","            \n","np.savetxt(root + name + 'newdata.csv', newdata, delimiter=\",\")\n","\n","np.savetxt(root + name + 'newlabels.csv', newlabels, delimiter=\",\")"],"execution_count":7,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4eac4110e9f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnewlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnwanted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnewlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnwanted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"BL7WBc_f0bW9","colab_type":"code","colab":{}},"cell_type":"code","source":["nimages=60000\n","temp = 4*np.ones((nimages,1))\n","fixed_y_ = onehot[temp.astype(np.int32)].squeeze()\n","test_images=sess.run(G_z, {z: np.random.normal(0, 1, (nimages, 100)), y: fixed_y_, isTrain: False})\n","\n","np.savetxt(root + name  +\"60000imgs.csv\", test_images, delimiter=\",\")\n","np.savetxt(root + name  +\"60000labs.csv\", fixed_y_, delimiter=\",\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lahZQnXtEJKB","colab_type":"code","colab":{}},"cell_type":"code","source":["n = np.random.randint(0,len(newlabels))\n","print(newlabels[n])\n","plt.imshow(np.reshape(newdata[n], (28, 28)), cmap='gray')"],"execution_count":0,"outputs":[]}]}